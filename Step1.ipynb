{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Generation of personalized genomes and chain files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a diploid personalized genome per samples by inserting phased variants from whole genomes (single nucleotide variants, small insertions and deletions, and structural variants) to reference genome (hg38)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load pigz Java\n",
    "module load Java/jdk1.8.0_151\n",
    "module load SAMtools/1.8-foss-2014a\n",
    "module load vt/20180405-foss-2014a\n",
    "\n",
    "SnpSift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this field for your sample\n",
    "sampleID=\"MM031\"\n",
    "input_folder=\"/staging/leuven/stg_00002/lcb/zkalender/Runs/MWGS_FINAL_SEQ_DATA/analysis/\"\n",
    "output_folder=/staging/leuven/stg_00002/lcb/zkalender/melanoma_WGS/${sampleID}\n",
    "resources_folder=\"/staging/leuven/stg_00002/lcb/resources/longranger/refdata-GRCh38-2.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary folders output folders\n",
    "mkdir -p ${output_folder}/crossstich\n",
    "mkdir -p ${output_folder}/snpeff\n",
    "mkdir -p ${resources_folder}/chrFiles/split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input files for crossstich pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. re-call SVs using sniffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first add MD tags to the bam\n",
    "samtools calmd \\\n",
    "    -@ 30 \\\n",
    "    -b ${input_folder}/${sampleID}/outs/phased_possorted_bam.bam \\\n",
    "    ${resources_folder}/fasta/genome.fa \\\n",
    "  > ${output_folder}/${sampleID}_phased_possorted_bam_MDtag.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sniffles\n",
    "sniffles \\\n",
    "    -m ${output_folder}/crossstich/${sampleID}_phased_possorted_bam_MDtag.bam \\\n",
    "    -v ${output_folder}/crossstich/${sampleID}.sniffles.n1.vcf \\\n",
    "    -n 1 \\\n",
    "    -t 30 \\\n",
    "  > ${output_folder}/crossstich/${sampleID}.sniffles.n1.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. filter SNVs & INDELs (for quality and genotype info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out variants w/o PASS tag and DP below 20\n",
    "java -jar SnpSift.jar \\\n",
    "    filter \"(FILTER = 'PASS') & (DP >=20)\" \\\n",
    "    ${input_folder}/${sampleID}/outs/phased_variants.vcf.gz \\\n",
    "  > ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose multi-allelic variants\n",
    "vt decompose \\\n",
    "    -s ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.vcf \\\n",
    "    -o ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out variants with missing genotype calls\n",
    "bcftools view \\\n",
    "    -g '^miss' \\\n",
    "    -O v \\\n",
    "    -o ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.GTfilt.vcf \\\n",
    "    ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.vcf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: extract \"hairs\" using a modified HapCut2 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HapCut2 step 1/3\n",
    "extractHAIRS \\\n",
    "    --10X 1 \\\n",
    "    --mbq 0 \\\n",
    "    --bam ${input_folder}/${sampleID}/outs/phased_possorted_bam.bam \\\n",
    "    --VCF ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.GTfilt.vcf \\\n",
    "    --out ${output_folder}/crossstich/${sampleID}_unlinked_fragment_file.mbq0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HapCut2 step 2/3\n",
    "python3 \\\n",
    "    /staging/leuven/stg_00002/lcb/cflerin/software/HapCUT2/utilities/LinkFragments.py \\\n",
    "    --bam ${input_folder}/${sampleID}/outs/phased_possorted_bam.bam \\\n",
    "    --VCF ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.GTfilt.vcf \\\n",
    "    --fragments ${output_folder}/crossstich/${sampleID}_unlinked_fragment_file.mbq0 \\\n",
    "    --out ${output_folder}/crossstich/${sampleID}_linked_fragment_file.mbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HapCut2 step 3/3\n",
    "/staging/leuven/stg_00002/lcb/cflerin/software/HapCUT2/build/HAPCUT2 \\\n",
    "    --nf 1 \\\n",
    "    --fragments ${output_folder}/crossstich/${sampleID}_unlinked_fragment_file.mbq0 \\\n",
    "    --VCF ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.vcf \\\n",
    "    --output ${output_folder}/crossstich/${sampleID}_haplotype_output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: run crossstitch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /staging/leuven/stg_00002/lcb/zkalender/melanoma_WGS/${sampleID}/crossstich\n",
    "\n",
    "/staging/leuven/stg_00002/lcb/cflerin/software/crossstitch/src/crossstitch.sh \\\n",
    "    ${output_folder}/snpeff/${sampleID}_phased_variants.HQ.decomposed.vcf \\\n",
    "    ${output_folder}/crossstich/${sampleID}.sniffles.n1.vcf \\\n",
    "    ${output_folder}/crossstich/${sampleID}_phased_possorted_bam_MDtag.bam \\\n",
    "    ${resources_folder}/fasta/genome.fa \\\n",
    "    ${sampleID} \\\n",
    "    female \\\n",
    "    0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step creates two fasta files (hap1.fa and hap2.fa) and two chain files (refTohap1.chain and refTohap2.chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create hap1ToRef.chain and hap2ToRef.chain files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load Kent/20180816\n",
    "module load Parallel/20180622\n",
    "module load Anaconda/5-Python-3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ${resources_folder}/chrFiles/split\n",
    "mkdir -p ${resources_folder}/chrFiles/lift\n",
    "\n",
    "mkdir -p ${output_dir}/liftover_proc\n",
    "mkdir -p ${output_dir}/liftover_proc/psl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map HAP1 to reference genome\n",
    "Only to canonical chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blat ${output_dir}/crossstich/${sampleID}.alleleseq/${sampleID}.hap1.fa \\\n",
    "    ${resources_folder}/genome.fa \\\n",
    "    -t=dna \\\n",
    "    -q=dna \\\n",
    "    -tileSize=11 \\\n",
    "    -fastMap \\\n",
    "    -minIdentity=95 \\\n",
    "    -noHead \\\n",
    "    -minScore=100 \\\n",
    "    -ooc=${output_dir}/liftover_proc/${sampleID}.hap1.fa.ooc \\\n",
    "    ${output_dir}/liftover_proc/psl/hap1.psl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map HAP2 to reference genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blat ${output_dir}/crossstich/${sampleID}.alleleseq/${sampleID}.hap2.fa \\\n",
    "    ${resources_folder}/genome.fa \\\n",
    "    -t=dna \\\n",
    "    -q=dna \\\n",
    "    -tileSize=11 \\\n",
    "    -fastMap \\\n",
    "    -minIdentity=95 \\\n",
    "    -noHead \\\n",
    "    -minScore=100 \\\n",
    "    -ooc=${output_dir}/liftover_proc/${sampleID}.hap2.fa.ooc \\\n",
    "    ${output_dir}/liftover_proc/psl/hap2.psl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is computationally intensive and can take quite a long time. One workaround is to split the reference genome into smaller chunks, and running blat jobs in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative mapping strategy: Split target assembly (hg38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ${resources_folder}/chrFiles folder contains a fasta file per canonical chromosome\n",
    "# below command will split each chr*.fa file into 3000 files\n",
    "\n",
    "for i in {1..22} X Y ; do\n",
    "    faSplit size \\\n",
    "        ${resources_folder}/chrFiles/chr${i}.fasta \\\n",
    "        3000 \\\n",
    "        ${resources_folder}/chrFiles/split/hg38.${i}.split \\\n",
    "        -lift=${resources_folder}/chrFiles/lift/hg38.${i}.lft \\\n",
    "        -oneFile;\n",
    "done\n",
    "\n",
    "# will split these files even further into 5k chunks\n",
    "\n",
    "source activate seqkit\n",
    "\n",
    "for file in `ls ${resources_folder}/chrFiles/split/` ; do\n",
    "    seqkit split ${resources_folder}/chrFiles/split/${file} \\\n",
    "        -s 5000 \\\n",
    "        -O ${resources_folder}/chrFiles/split/5k_3k_fa/${file};\n",
    "done\n",
    "\n",
    "# map hap1 to split fasta files\n",
    "cat ${resources_folder}/hg38_liftover_process_files/5k_3k_fa_file_list \\\n",
    "  | parallel -j 30 \\\n",
    "        blat ${sampleID}.hap1.fa \\\n",
    "            ${resources_folder}/hg38_liftover_process_files/5k_3k_fa/{} \\\n",
    "            -t=dna \\\n",
    "            -q=dna \\\n",
    "            -tileSize=11 \\\n",
    "            -fastMap \\\n",
    "            -minIdentity=95 \\\n",
    "            -noHead \\\n",
    "            -minScore=100 \\\n",
    "            -ooc=${sampleID}.hap1.fa.ooc \\\n",
    "            psl/hap1.{}.psl\n",
    "\n",
    "\n",
    "# map hap2 to split fasta files\n",
    "cat ${resources_folder}/hg38_liftover_process_files/5k_3k_fa_file_list \\\n",
    "  | parallel -j 30 \\\n",
    "        blat ${sampleID}.hap2.fa \\\n",
    "            ${resources_folder}/hg38_liftover_process_files/5k_3k_fa/{} \\\n",
    "            -t=dna \\\n",
    "            -q=dna \\\n",
    "            -tileSize=11 \\\n",
    "            -fastMap \\\n",
    "            -minIdentity=95 \\\n",
    "            -noHead \\\n",
    "            -minScore=100 \\\n",
    "            -ooc=${sampleID}.hap2.fa.ooc \\\n",
    "            psl/hap2.{}.psl\n",
    "\n",
    "# Combine psl files\n",
    "cat ${resources_folder}/hg38_liftover_process_files/5k_3k_fa_file_list | while read file; do cat psl/hap1.${file}.psl ; done >> psl/hap1.psl\n",
    "cat ${resources_folder}/hg38_liftover_process_files/5k_3k_fa_file_list | while read file; do cat psl/hap2.${file}.psl ; done >> psl/hap2.psl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create chain files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# litftup\n",
    "liftUp -pslQ liftup/hap1.combined.liftup.psl ${resources_folder}/hg38_liftover_process_files/lift/hg38.combined.lft warn psl/hap1.psl\n",
    "liftUp -pslQ liftup/hap2.combined.liftup.psl ${resources_folder}/hg38_liftover_process_files/lift/hg38.combined.lft warn psl/hap2.psl\n",
    "\n",
    "# make chain files\n",
    "mkdir chain_raw\n",
    "\n",
    "axtChain \\\n",
    "    -linearGap=medium \\\n",
    "    -faQ \\\n",
    "    -faT \\\n",
    "    -psl liftup/hap1.combined.liftup.psl \\\n",
    "    ${sample}.hap1.fa \\\n",
    "    ${resources_folder}/hg38_liftover_process_files/hg38_canonical_chromosomes.fa \\\n",
    "    chain_raw/${sample}.hap1.chain\n",
    "\n",
    "axtChain \\\n",
    "    -linearGap=medium \\\n",
    "    -faQ \\\n",
    "    -faT \\\n",
    "    -psl liftup/hap2.combined.liftup.psl \\\n",
    "    ${sample}.hap2.fa \\\n",
    "    ${resources_folder}/hg38_liftover_process_files/hg38_canonical_chromosomes.fa \\\n",
    "    chain_raw/${sample}.hap2.chain\n",
    "\n",
    "# merge and sort chain files\n",
    "chainMergeSort chain_raw/${sample}.hap1.chain | chainSplit chain_split.hap1 stdin\n",
    "chainMergeSort chain_raw/${sample}.hap2.chain | chainSplit chain_split.hap2 stdin\n",
    "\n",
    "faSize ${sample}.hap1.fa -detailed > ${sample}.hap1.chr_length.txt\n",
    "faSize ${sample}.hap2.fa -detailed > ${sample}.hap2.chr_length.txt\n",
    "\n",
    "# make alignment nets from chain files\n",
    "mkdir net\n",
    "\n",
    "for i in chain_split.hap1/*.chain ; do\n",
    "    tag=${i#chain_split.hap1/};\n",
    "    chainNet ${i} ${sample}.hap1.chr_length.txt ${resources_folder}/hg38_liftover_process_files/hg38_canonical_chromosomes.chr_length.txt net/${tag}.net /dev/null\n",
    "done\n",
    "\n",
    "for i in chain_split.hap2/*.chain ; do\n",
    "    tag=${i#chain_split.hap2/};\n",
    "    chainNet ${i} ${sample}.hap2.chr_length.txt ${resources_folder}/hg38_liftover_process_files/hg38_canonical_chromosomes.chr_length.txt net/${tag}.net /dev/null\n",
    "done\n",
    "\n",
    "# create liftOver chain file\n",
    "mkdir over\n",
    "for i in chain_split.hap1/*.chain ; do\n",
    "    tag=${i#chain_split.hap1/};\n",
    "    netChainSubset net/${tag}.net ${i} over/${tag}\n",
    "done\n",
    "\n",
    "for i in chain_split.hap2/*.chain ; do\n",
    "    tag=${i#chain_split.hap2/};\n",
    "    netChainSubset net/${tag}.net ${i} over/${tag}\n",
    "done\n",
    "\n",
    "cat over/*hap1.chain > ${sample}.hap1_to_hg38.over.chain\n",
    "cat over/*hap2.chain > ${sample}.hap2_to_hg38.over.chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
