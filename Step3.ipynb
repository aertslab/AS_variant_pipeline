{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. Obtain allele specific counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add detailed description here \n",
    "\n",
    "ASB detection and BaalChIP procedure is also done here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary modules \n",
    "module load SAMtools/1.9-20181106-foss-2014a\n",
    "module load bedtools/20181008-foss-2014a\n",
    "module load Kent/20180816\n",
    "module load Picard/2.18.11-Java-jdk1.8.0_151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give sample name here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create haplotype specific bam files \n",
    "/staging/leuven/stg_00002/lcb/zkalender/software/src_zkalender/melanoma_WGS_project/postprocess_hap1_and_hap2_bam_files_v2.sh ${sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark duplicates in unique and commonly mapping reads\n",
    "picard MarkDuplicates \\\n",
    "I=${sample}_HAP1.common.bam \\\n",
    "O=${sample}_HAP1.common.dup_marked.bam \\\n",
    "CREATE_INDEX=false \\\n",
    "VALIDATION_STRINGENCY=SILENT \\\n",
    "REMOVE_DUPLICATES=FALSE \\\n",
    "M=${sample}_HAP1.common.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "I=${sample}_HAP2.common.bam \\\n",
    "O=${sample}_HAP2.common.dup_marked.bam \\\n",
    "CREATE_INDEX=false \\\n",
    "VALIDATION_STRINGENCY=SILENT \\\n",
    "REMOVE_DUPLICATES=FALSE \\\n",
    "M=${sample}_HAP2.common.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "I=${sample}_HAP1.unique.bam \\\n",
    "O=${sample}_HAP1.unique.dup_marked.bam \\\n",
    "CREATE_INDEX=false \\\n",
    "VALIDATION_STRINGENCY=SILENT \\\n",
    "REMOVE_DUPLICATES=FALSE \\\n",
    "M=${sample}_HAP1.unique.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "I=${sample}_HAP2.unique.bam \\\n",
    "O=${sample}_HAP2.unique.dup_marked.bam \\\n",
    "CREATE_INDEX=false \\\n",
    "VALIDATION_STRINGENCY=SILENT \\\n",
    "REMOVE_DUPLICATES=FALSE \\\n",
    "M=${sample}_HAP2.unique.dedup.metrics &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of duplicate reads\n",
    "echo HAP1_common.bam has `cat ${sample}_HAP1.common.dedup.metrics | awk 'NR==8'| cut -f2` reads, `cat ${sample}_HAP1.common.dedup.metrics | awk 'NR==8'| cut -f6` of which are duplicates\n",
    "echo HAP2_common.bam has `cat ${sample}_HAP2.common.dedup.metrics | awk 'NR==8'| cut -f2` reads, `cat ${sample}_HAP2.common.dedup.metrics | awk 'NR==8'| cut -f6` of which are duplicates\n",
    "echo HAP1_unique.bam has `cat ${sample}_HAP1.unique.dedup.metrics | awk 'NR==8'| cut -f2` reads, `cat ${sample}_HAP1.unique.dedup.metrics | awk 'NR==8'| cut -f6` of which are duplicates\n",
    "echo HAP2_unique.bam has `cat ${sample}_HAP2.unique.dedup.metrics | awk 'NR==8'| cut -f2` reads, `cat ${sample}_HAP2.unique.dedup.metrics | awk 'NR==8'| cut -f6` of which are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liftover common ones to hg38 - check how many are mapping exactly at the same spot\n",
    "bamToBed -i ${sample}_HAP1.common.bam > ${sample}_HAP1.common.bed &\n",
    "bamToBed -i ${sample}_HAP2.common.bam > ${sample}_HAP2.common.bed &\n",
    "\n",
    "wait\n",
    "\n",
    "liftOver ${sample}_HAP1.common.bed ${sample}_HAP1_to_HG38.chain ${sample}_HAP1.common.hg38.bed unmapp1 &\n",
    "liftOver ${sample}_HAP2.common.bed ${sample}_HAP2_to_HG38.chain ${sample}_HAP2.common.hg38.bed unmapp2 &\n",
    "\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of reads that couldn't be lifted over \n",
    "echo Out of `cat ${sample}_HAP1.common.bed| wc -l` HAP1.common reads `cat unmapp1 | grep -v '^#' | wc -l` could not be lifted over to hg38 >> postprocess_haplotype_bam_files.log\n",
    "echo Out of `cat ${sample}_HAP2.common.bed| wc -l` HAP1.common reads `cat unmapp2 | grep -v '^#' | wc -l` could not be lifted over to hg38 >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unambigously mapping reads \n",
    "intersectBed -a ${sample}_HAP1.common.hg38.bed -b ${sample}_HAP2.common.hg38.bed -r -f 1 -wa > ${sample}_HAP1_and_HAP2_common_reads\n",
    "cat ${sample}_HAP1_and_HAP2_common_reads | cut -f4| sort -u > ${sample}_HAP1_and_HAP2_common_read.IDs\n",
    "\n",
    "# report the number of unambigously mapping reads \n",
    "echo `cat ${sample}_HAP1_and_HAP2_common_read.IDs | wc -l` reads are unambigously mapping between ${sample}_HAP1.common and ${sample}_HAP2.common >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out reads mapped to different locations\n",
    "picard FilterSamReads I=${sample}_HAP1.common.bam O=${sample}_HAP1.common.no_amb.bam FILTER=includeReadList READ_LIST_FILE=${sample}_HAP1_and_HAP2_common_read.IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.common.bam O=${sample}_HAP2.common.no_amb.bam FILTER=includeReadList READ_LIST_FILE=${sample}_HAP1_and_HAP2_common_read.IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out reads that are identified as duplicates (in either of the common.bam files)\n",
    "samtools view ${sample}_HAP1.common.dup_marked.bam | awk '$2==1040|| $2==1024' | cut -f1 | sort -u > ${sample}_HAP1.common.dup_read_IDs\n",
    "samtools view ${sample}_HAP2.common.dup_marked.bam | awk '$2==1040|| $2==1024' | cut -f1 | sort -u > ${sample}_HAP2.common.dup_read_IDs\n",
    "cat ${sample}_HAP1.common.dup_read_IDs ${sample}_HAP2.common.dup_read_IDs | sort -u > ${sample}.common.dup_read_IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the total number of duplicated reads\n",
    "echo There are collectively `cat ${sample}.common.dup_read_IDs | wc -l` duplicate reads in ${sample}_HAP1.common and ${sample}_HAP2.common >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicated reads \n",
    "picard FilterSamReads I=${sample}_HAP1.common.no_amb.bam O=${sample}_HAP1.common.no_amb.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}.common.dup_read_IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.common.no_amb.bam O=${sample}_HAP2.common.no_amb.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}.common.dup_read_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pileup over heterozygous sites\n",
    "# extract necessay fields from WGS genome vcf file\n",
    "java -Xmx100G -jar /data/leuven/software/biomed/SnpEff/4.3p/SnpSift.jar extractFields \\\n",
    "/staging/leuven/stg_00002/lcb/zkalender/melanoma_WGS/${sample}/snpeff/${sample}_phased_variants.HQ.ann_dbNSFP3.snp150.COSMIC.vcf.gz \\\n",
    "-s '|' \\\n",
    "-e '.' CHROM POS ID REF ALT DP GEN[*].AD GEN[*].PS GEN[*].GT AF  > ${sample}_WGS_variants_with_extra_info\n",
    "\n",
    "sed 1d ${sample}_WGS_variants_with_extra_info |awk '{print $1\"\\t\"$2-1\"\\t\"$2\"\\t\"$4\"/\"$5}' |\n",
    "intersectBed -a stdin -b /staging/leuven/stg_00002/lcb/zkalender/melanoma_WGS/${sample}/AS_ATAC/${sample}_REF_HAP1_HAP2_peaks.merged_and_filtered.bed -wa \\\n",
    "> ${sample}_HET_SNVs.hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of heterozygous sites that are considered for allele-specific activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo Pileup over heterozygous sites >> postprocess_haplotype_bam_files.log\n",
    "echo There are `cat ${sample}_HET_SNVs.hg38.bed | wc -l` heterozygous sites in the genome >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liftover het sites to hap1 and hap2\n",
    "liftOver ${sample}_HET_SNVs.hg38.bed ${sample}.refTOhap1.chain ${sample}_HET_SNVs.hap1.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hg38.bed ${sample}.refTOhap2.chain ${sample}_HET_SNVs.hap2.bed unmapp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicate reads from HAP1 and HAP1 mapped reads \n",
    "samtools view ${sample}_HAP1.unique.dup_marked.bam | awk '$2==1040|| $2==1024' | cut -f1 | sort -u > ${sample}_HAP1.unique.dup_read_IDs\n",
    "samtools view ${sample}_HAP2.unique.dup_marked.bam | awk '$2==1040|| $2==1024' | cut -f1 | sort -u > ${sample}_HAP2.unique.dup_read_IDs\n",
    "\n",
    "picard FilterSamReads I=${sample}_HAP1.unique.bam O=${sample}_HAP1.unique.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}_HAP1.unique.dup_read_IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.unique.bam O=${sample}_HAP2.unique.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}_HAP2.unique.dup_read_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of used reads for allele counting \n",
    "echo Number of reads used for pileup: >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP1.unique:`samtools view ${sample}_HAP1.unique.no_dup.bam| cut -f1|sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP2.unique:`samtools view ${sample}_HAP2.unique.no_dup.bam| cut -f1|sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP1.common:`samtools view ${sample}_HAP1.common.no_amb.no_dup.bam| cut -f1|sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP2.common:`samtools view ${sample}_HAP2.common.no_amb.no_dup.bam| cut -f1|sort -u | wc -l ` >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pileup using unique reads\n",
    "# hap1\n",
    "samtools mpileup -Bf ${sample}.hap1.fa -l ${sample}_HET_SNVs.hap1.bed ${sample}_HAP1.unique.no_dup.bam > ${sample}_HET_SNVs.hap1_unique.pileup &\n",
    "# hap2\n",
    "samtools mpileup -Bf ${sample}.hap2.fa -l ${sample}_HET_SNVs.hap2.bed ${sample}_HAP2.unique.no_dup.bam > ${sample}_HET_SNVs.hap2_unique.pileup &\n",
    "\n",
    "wait\n",
    "\n",
    "# pileup using common reads\n",
    "#hap1\n",
    "samtools mpileup -Bf ${sample}.hap1.fa -l ${sample}_HET_SNVs.hap1.bed ${sample}_HAP1.common.no_amb.no_dup.bam > ${sample}_HET_SNVs.hap1_common.pileup &\n",
    "# hap2\n",
    "samtools mpileup -Bf ${sample}.hap2.fa -l ${sample}_HET_SNVs.hap2.bed ${sample}_HAP2.common.no_amb.no_dup.bam > ${sample}_HET_SNVs.hap2_common.pileup &\n",
    "\n",
    "wait\n",
    "\n",
    "# generate counts from pileup files\n",
    "cat ${sample}_HET_SNVs.hap1_unique.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk >  ${sample}_HET_SNVs.hap1_unique.counts.bed\n",
    "cat ${sample}_HET_SNVs.hap2_unique.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk >  ${sample}_HET_SNVs.hap2_unique.counts.bed\n",
    "\n",
    "cat ${sample}_HET_SNVs.hap1_common.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk >  ${sample}_HET_SNVs.hap1_common.counts.bed\n",
    "cat ${sample}_HET_SNVs.hap2_common.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk >  ${sample}_HET_SNVs.hap2_common.counts.bed\n",
    "\n",
    "# liftover everything back to hg38\n",
    "liftOver ${sample}_HET_SNVs.hap1_unique.counts.bed ${sample}_HAP1_to_HG38.chain ${sample}_HET_SNVs.hap1_unique.counts.hg38.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hap1_common.counts.bed ${sample}_HAP1_to_HG38.chain ${sample}_HET_SNVs.hap1_common.counts.hg38.bed unmapp1\n",
    "\n",
    "liftOver ${sample}_HET_SNVs.hap2_unique.counts.bed ${sample}_HAP2_to_HG38.chain ${sample}_HET_SNVs.hap2_unique.counts.hg38.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hap2_common.counts.bed ${sample}_HAP2_to_HG38.chain ${sample}_HET_SNVs.hap2_common.counts.hg38.bed unmapp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out INDELs and homozygous variants AND Overlap with ATAC peaks\n",
    "sed 1d ${sample}_WGS_variants_with_extra_info | \\\n",
    "awk '(length($4)==1 && length($5)==1)' | awk '$9==\"0|1\" || $9==\"1|0\"' | awk '{print $1,$2-1,$2,$3,$4,$5,$6,$7,$9,$10}' | \\\n",
    "tr ' ' '\\t' | intersectBed -a stdin \\\n",
    "-b /staging/leuven/stg_00002/lcb/zkalender/melanoma_WGS/${sample}/AS_ATAC/${sample}_REF_HAP1_HAP2_peaks.merged_and_filtered.bed \\\n",
    "-wo | cut -f1-15 | sortBed -i stdin | awk '{print $0\"\\t'${sample}'_SNV_\"NR}' > heterozygote_SNVs_in_peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat heterozygote_SNVs_in_peaks.bed | cut -f1-3,16 > heterozygote_SNVs_in_peaks.NAMED.bed\n",
    "\n",
    "echo Generating counts >> postprocess_haplotype_bam_files.log\n",
    "\n",
    "# intersect the named bed file with unique files\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap1_unique.counts.hg38.bed | intersectBed -a heterozygote_SNVs_in_peaks.NAMED.bed -b stdin -wo | cut -f4,8 > HET_SNVs_hap1_counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap2_unique.counts.hg38.bed | intersectBed -a heterozygote_SNVs_in_peaks.NAMED.bed -b stdin -wo | cut -f4,8 > HET_SNVs_hap2_counts\n",
    "\n",
    "# intersect the named bed file with common counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap1_common.counts.hg38.bed| intersectBed -a stdin -b heterozygote_SNVs_in_peaks.NAMED.bed -wo | cut -f4,8 > HET_SNVs_hap1_common_counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap2_common.counts.hg38.bed| intersectBed -a stdin -b heterozygote_SNVs_in_peaks.NAMED.bed -wo | cut -f4,8 > HET_SNVs_hap2_common_counts\n",
    "\n",
    "echo Done >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# combine allelic counts at heterozygous sites using HAP1-specific, HAP2-specific and commonly mapped reads\n",
    "Rscript alleleseq_combine_counts.R "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# add genomic allelic ratios for BaalChIP\n",
    "sed 1d $file/alleleseq_bowtie2/m_min6.txt | awk 'OFS=\"\\t\" {print $2,$3,$4,$1,$21,$22,$23}'|intersectBed -a stdin -b  $file/snpeff/${file}_phased_variants.HQ.pileup_with_WGS -wo > $file/alleleseq_bowtie2/BaalChIP/m_min6_with_RAF_for_BaalChIP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rscript BaalChIP_process.R ${sample}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
