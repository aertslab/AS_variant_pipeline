{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. Obtaining allele specific counts and predict ASCAVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we aim to obtain allele specific counts. We first start by comparing HAP1 and HAP2 bam files generated in the first step.\n",
    "We evaluate each read in these bam files in order to identify its most likely origin. We do this by looking first at mapping quality, then at the number of mismatches. If a read has higher mapping quality for either of the haplotypes it is assigned to that haplotype. If the mapping quality is the same, we look at the number of mismatches and assigned the read to the haplotype for which the read is aligned with least number of mismatches. If both metrics are equivalent, we deem the read as commonly mapping to both haplotypes. This step results in four bam files: HAP1.unique.bam, HAP2.unique.bam, HAP1.common.bam and HAP2.common.bam (HAP1.common.bam and HAP2.common.bam should essentially be identical).\n",
    "We next check duplicate and ambiguously mapping reads. We mark the duplicate reads in each of these bam files with Picard. Then, by lifting over the read positions in HAP1.common.bam and HAP2.common.bam to reference genome, we check for ambiguously mapping reads (if the reads are truly indistinguishable between two haplotypes, they should correspond to the same location when lifted over to the reference genome).\n",
    "Next, we obtain phased heterozygous variants from whole genome sequence data and overlap them with consolidated peaks obtained in the previous step. These variants will be the ones that we will try to obtain allele specific counts. SNV locations in peaks are lifted over to HAP1 and HAP2, and we count the alleles at these variants using HAP1.bam and HAP2.bam files with samtools mpileup. We also count the alleles using the COMMON.bam file.\n",
    "Next, we combine allelic counts from HAP1, HAP2 and COMMON alignments, add genomic allelic ratios from WGS data, and predict allele specific chromatin accessibility variants using BaalCHIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary modules\n",
    "module load SAMtools/1.9-20181106-foss-2014a\n",
    "module load bedtools/20181008-foss-2014a\n",
    "module load Kent/20180816\n",
    "module load Picard/2.18.11-Java-jdk1.8.0_151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give sample name here\n",
    "sample=\"MM031\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create haplotype specific bam files\n",
    "/staging/leuven/stg_00002/lcb/zkalender/software/src_zkalender/melanoma_WGS_project/postprocess_hap1_and_hap2_bam_files_v2.sh ${sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark duplicates in unique and commonly mapping reads\n",
    "picard MarkDuplicates \\\n",
    "    I=${sample}_HAP1.common.bam \\\n",
    "    O=${sample}_HAP1.common.dup_marked.bam \\\n",
    "    CREATE_INDEX=false \\\n",
    "    VALIDATION_STRINGENCY=SILENT \\\n",
    "    REMOVE_DUPLICATES=FALSE \\\n",
    "    M=${sample}_HAP1.common.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "    I=${sample}_HAP2.common.bam \\\n",
    "    O=${sample}_HAP2.common.dup_marked.bam \\\n",
    "    CREATE_INDEX=false \\\n",
    "    VALIDATION_STRINGENCY=SILENT \\\n",
    "    REMOVE_DUPLICATES=FALSE \\\n",
    "    M=${sample}_HAP2.common.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "    I=${sample}_HAP1.unique.bam \\\n",
    "    O=${sample}_HAP1.unique.dup_marked.bam \\\n",
    "    CREATE_INDEX=false \\\n",
    "    VALIDATION_STRINGENCY=SILENT \\\n",
    "    REMOVE_DUPLICATES=FALSE \\\n",
    "    M=${sample}_HAP1.unique.dedup.metrics &\n",
    "\n",
    "\n",
    "picard MarkDuplicates \\\n",
    "    I=${sample}_HAP2.unique.bam \\\n",
    "    O=${sample}_HAP2.unique.dup_marked.bam \\\n",
    "    CREATE_INDEX=false \\\n",
    "    VALIDATION_STRINGENCY=SILENT \\\n",
    "    REMOVE_DUPLICATES=FALSE \\\n",
    "    M=${sample}_HAP2.unique.dedup.metrics &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of duplicate reads\n",
    "echo HAP1_common.bam has `cat ${sample}_HAP1.common.dedup.metrics | awk 'NR==8' | cut -f2` reads, `cat ${sample}_HAP1.common.dedup.metrics | awk 'NR==8' | cut -f6` of which are duplicates\n",
    "echo HAP2_common.bam has `cat ${sample}_HAP2.common.dedup.metrics | awk 'NR==8' | cut -f2` reads, `cat ${sample}_HAP2.common.dedup.metrics | awk 'NR==8' | cut -f6` of which are duplicates\n",
    "echo HAP1_unique.bam has `cat ${sample}_HAP1.unique.dedup.metrics | awk 'NR==8' | cut -f2` reads, `cat ${sample}_HAP1.unique.dedup.metrics | awk 'NR==8' | cut -f6` of which are duplicates\n",
    "echo HAP2_unique.bam has `cat ${sample}_HAP2.unique.dedup.metrics | awk 'NR==8' | cut -f2` reads, `cat ${sample}_HAP2.unique.dedup.metrics | awk 'NR==8' | cut -f6` of which are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liftover common ones to hg38 - check how many are mapping exactly at the same spot\n",
    "bamToBed -i ${sample}_HAP1.common.bam > ${sample}_HAP1.common.bed &\n",
    "bamToBed -i ${sample}_HAP2.common.bam > ${sample}_HAP2.common.bed &\n",
    "\n",
    "wait\n",
    "\n",
    "liftOver ${sample}_HAP1.common.bed ${sample}_HAP1_to_HG38.chain ${sample}_HAP1.common.hg38.bed unmapp1 &\n",
    "liftOver ${sample}_HAP2.common.bed ${sample}_HAP2_to_HG38.chain ${sample}_HAP2.common.hg38.bed unmapp2 &\n",
    "\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of reads that couldn't be lifted over\n",
    "echo Out of `cat ${sample}_HAP1.common.bed | wc -l` HAP1.common reads `cat unmapp1 | grep -v '^#' | wc -l` could not be lifted over to hg38 >> postprocess_haplotype_bam_files.log\n",
    "echo Out of `cat ${sample}_HAP2.common.bed | wc -l` HAP1.common reads `cat unmapp2 | grep -v '^#' | wc -l` could not be lifted over to hg38 >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unambigously mapping reads\n",
    "intersectBed -a ${sample}_HAP1.common.hg38.bed -b ${sample}_HAP2.common.hg38.bed -r -f 1 -wa > ${sample}_HAP1_and_HAP2_common_reads\n",
    "cat ${sample}_HAP1_and_HAP2_common_reads | cut -f4 | sort -u > ${sample}_HAP1_and_HAP2_common_read.IDs\n",
    "\n",
    "# report the number of unambigously mapping reads\n",
    "echo `cat ${sample}_HAP1_and_HAP2_common_read.IDs | wc -l` reads are unambigously mapping between ${sample}_HAP1.common and ${sample}_HAP2.common >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out reads mapped to different locations\n",
    "picard FilterSamReads I=${sample}_HAP1.common.bam O=${sample}_HAP1.common.no_amb.bam FILTER=includeReadList READ_LIST_FILE=${sample}_HAP1_and_HAP2_common_read.IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.common.bam O=${sample}_HAP2.common.no_amb.bam FILTER=includeReadList READ_LIST_FILE=${sample}_HAP1_and_HAP2_common_read.IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out reads that are identified as duplicates (in either of the common.bam files)\n",
    "samtools view ${sample}_HAP1.common.dup_marked.bam | awk '$2==1040 || $2==1024' | cut -f1 | sort -u > ${sample}_HAP1.common.dup_read_IDs\n",
    "samtools view ${sample}_HAP2.common.dup_marked.bam | awk '$2==1040 || $2==1024' | cut -f1 | sort -u > ${sample}_HAP2.common.dup_read_IDs\n",
    "cat ${sample}_HAP1.common.dup_read_IDs ${sample}_HAP2.common.dup_read_IDs | sort -u > ${sample}.common.dup_read_IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the total number of duplicated reads\n",
    "echo There are collectively `cat ${sample}.common.dup_read_IDs | wc -l` duplicate reads in ${sample}_HAP1.common and ${sample}_HAP2.common >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicated reads\n",
    "picard FilterSamReads I=${sample}_HAP1.common.no_amb.bam O=${sample}_HAP1.common.no_amb.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}.common.dup_read_IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.common.no_amb.bam O=${sample}_HAP2.common.no_amb.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}.common.dup_read_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pileup over heterozygous sites\n",
    "# extract necessay fields from WGS genome vcf file\n",
    "java -Xmx100G -jar /data/leuven/software/biomed/SnpEff/4.3p/SnpSift.jar extractFields \\\n",
    "    ${wgs_folder}/${sample}/snpeff/${sample}_phased_variants.HQ.ann_dbNSFP3.snp150.COSMIC.vcf.gz \\\n",
    "    -s '|' \\\n",
    "    -e '.' CHROM POS ID REF ALT DP GEN[*].AD GEN[*].PS GEN[*].GT AF \\\n",
    "  > ${sample}_WGS_variants_with_extra_info\n",
    "\n",
    "sed 1d ${sample}_WGS_variants_with_extra_info \\\n",
    "    | awk '{print $1\"\\t\"$2-1\"\\t\"$2\"\\t\"$4\"/\"$5}' \\\n",
    "    | intersectBed -a stdin -b ${wgs_folder}/${sample}/AS_ATAC/${sample}_REF_HAP1_HAP2_peaks.merged_and_filtered.bed -wa \\\n",
    "  > ${sample}_HET_SNVs.hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of heterozygous sites that are considered for allele-specific activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo Pileup over heterozygous sites >> postprocess_haplotype_bam_files.log\n",
    "echo There are `cat ${sample}_HET_SNVs.hg38.bed | wc -l` heterozygous sites in the genome >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liftover het sites to hap1 and hap2\n",
    "liftOver ${sample}_HET_SNVs.hg38.bed ${sample}.refTOhap1.chain ${sample}_HET_SNVs.hap1.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hg38.bed ${sample}.refTOhap2.chain ${sample}_HET_SNVs.hap2.bed unmapp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicate reads from HAP1 and HAP1 mapped reads\n",
    "samtools view ${sample}_HAP1.unique.dup_marked.bam | awk '$2==1040 || $2==1024' | cut -f1 | sort -u > ${sample}_HAP1.unique.dup_read_IDs\n",
    "samtools view ${sample}_HAP2.unique.dup_marked.bam | awk '$2==1040 || $2==1024' | cut -f1 | sort -u > ${sample}_HAP2.unique.dup_read_IDs\n",
    "\n",
    "picard FilterSamReads I=${sample}_HAP1.unique.bam O=${sample}_HAP1.unique.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}_HAP1.unique.dup_read_IDs\n",
    "picard FilterSamReads I=${sample}_HAP2.unique.bam O=${sample}_HAP2.unique.no_dup.bam FILTER=excludeReadList READ_LIST_FILE=${sample}_HAP2.unique.dup_read_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of used reads for allele counting\n",
    "echo Number of reads used for pileup: >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP1.unique:`samtools view ${sample}_HAP1.unique.no_dup.bam | cut -f1 |sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP2.unique:`samtools view ${sample}_HAP2.unique.no_dup.bam | cut -f1 |sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP1.common:`samtools view ${sample}_HAP1.common.no_amb.no_dup.bam | cut -f1 |sort -u | wc -l ` >> postprocess_haplotype_bam_files.log\n",
    "echo ${sample}_HAP2.common:`samtools view ${sample}_HAP2.common.no_amb.no_dup.bam | cut -f1 |sort -u | wc -l ` >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pileup using unique reads\n",
    "# hap1\n",
    "samtools mpileup -Bf ${sample}.hap1.fa -l ${sample}_HET_SNVs.hap1.bed ${sample}_HAP1.unique.no_dup.bam > ${sample}_HET_SNVs.hap1_unique.pileup &\n",
    "# hap2\n",
    "samtools mpileup -Bf ${sample}.hap2.fa -l ${sample}_HET_SNVs.hap2.bed ${sample}_HAP2.unique.no_dup.bam > ${sample}_HET_SNVs.hap2_unique.pileup &\n",
    "\n",
    "wait\n",
    "\n",
    "# pileup using common reads\n",
    "#hap1\n",
    "samtools mpileup -Bf ${sample}.hap1.fa -l ${sample}_HET_SNVs.hap1.bed ${sample}_HAP1.common.no_amb.no_dup.bam > ${sample}_HET_SNVs.hap1_common.pileup &\n",
    "# hap2\n",
    "samtools mpileup -Bf ${sample}.hap2.fa -l ${sample}_HET_SNVs.hap2.bed ${sample}_HAP2.common.no_amb.no_dup.bam > ${sample}_HET_SNVs.hap2_common.pileup &\n",
    "\n",
    "wait\n",
    "\n",
    "# generate counts from pileup files\n",
    "cat ${sample}_HET_SNVs.hap1_unique.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk > ${sample}_HET_SNVs.hap1_unique.counts.bed\n",
    "cat ${sample}_HET_SNVs.hap2_unique.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk > ${sample}_HET_SNVs.hap2_unique.counts.bed\n",
    "\n",
    "cat ${sample}_HET_SNVs.hap1_common.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk > ${sample}_HET_SNVs.hap1_common.counts.bed\n",
    "cat ${sample}_HET_SNVs.hap2_common.pileup | ~/lcb/zkalender/software/pileup2bed_depth_vaf_with_allele_counts.awk > ${sample}_HET_SNVs.hap2_common.counts.bed\n",
    "\n",
    "# liftover everything back to hg38\n",
    "liftOver ${sample}_HET_SNVs.hap1_unique.counts.bed ${sample}_HAP1_to_HG38.chain ${sample}_HET_SNVs.hap1_unique.counts.hg38.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hap1_common.counts.bed ${sample}_HAP1_to_HG38.chain ${sample}_HET_SNVs.hap1_common.counts.hg38.bed unmapp1\n",
    "\n",
    "liftOver ${sample}_HET_SNVs.hap2_unique.counts.bed ${sample}_HAP2_to_HG38.chain ${sample}_HET_SNVs.hap2_unique.counts.hg38.bed unmapp1\n",
    "liftOver ${sample}_HET_SNVs.hap2_common.counts.bed ${sample}_HAP2_to_HG38.chain ${sample}_HET_SNVs.hap2_common.counts.hg38.bed unmapp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out INDELs and homozygous variants AND Overlap with ATAC peaks\n",
    "sed 1d ${sample}_WGS_variants_with_extra_info \\\n",
    "  | awk -v 'OFS=\\t' '{ if (length($4)==1 && length($5)==1) { if ($9==\"0|1\" || $9==\"1|0\") { print $1, $2-1, $2, $3, $4, $5, $6, $7, $9, $10 } } }' \\\n",
    "  | intersectBed \\\n",
    "        -a stdin \\\n",
    "        -b ${wgs_folder}/${sample}/AS_ATAC/${sample}_REF_HAP1_HAP2_peaks.merged_and_filtered.bed \\\n",
    "        -wo \\\n",
    "  | cut -f 1-15 \\\n",
    "  | sortBed -i stdin \\\n",
    "  | awk '{print $0\"\\t'${sample}'_SNV_\"NR}' \\\n",
    "  > heterozygote_SNVs_in_peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat heterozygote_SNVs_in_peaks.bed | cut -f 1-3,16 > heterozygote_SNVs_in_peaks.NAMED.bed\n",
    "\n",
    "echo Generating counts >> postprocess_haplotype_bam_files.log\n",
    "\n",
    "# intersect the named bed file with unique files\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap1_unique.counts.hg38.bed | intersectBed -a heterozygote_SNVs_in_peaks.NAMED.bed -b stdin -wo | cut -f 4,8 > HET_SNVs_hap1_counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap2_unique.counts.hg38.bed | intersectBed -a heterozygote_SNVs_in_peaks.NAMED.bed -b stdin -wo | cut -f 4,8 > HET_SNVs_hap2_counts\n",
    "\n",
    "# intersect the named bed file with common counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap1_common.counts.hg38.bed | intersectBed -a stdin -b heterozygote_SNVs_in_peaks.NAMED.bed -wo | cut -f 4,8 > HET_SNVs_hap1_common_counts\n",
    "awk '$4!=0' ${sample}_HET_SNVs.hap2_common.counts.hg38.bed | intersectBed -a stdin -b heterozygote_SNVs_in_peaks.NAMED.bed -wo | cut -f 4,8 > HET_SNVs_hap2_common_counts\n",
    "\n",
    "echo Done >> postprocess_haplotype_bam_files.log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# combine allelic counts at heterozygous sites using HAP1-specific, HAP2-specific and commonly mapped reads\n",
    "Rscript alleleseq_combine_counts.R"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# obtain genomic allelic ratios from WGS data\n",
    "samtools mpileup \\\n",
    "    -B \\\n",
    "    -f ${resources_folder}/refdata-GRCh38-2.1.0/fasta/genome.fa \\\n",
    "    -l ${wgs_folder}/${sample}/snpeff/${sample}_phased_variants.HQ.bed \\\n",
    "    /staging/leuven/stg_00002/lcb/zkalender/Runs/MWGS_FINAL_SEQ_DATA/analysis/${sample}/outs/phased_possorted_bam.bam \\\n",
    "  | ~/lcb/zkalender/software/pileup2bed_depth_vaf.awk\\\n",
    "  > ${wgs_folder}/${sample}/snpeff/${sample}_phased_variants.HQ.pileup_with_WGS\n",
    "\n",
    "\n",
    "# add genomic allelic ratios for BaalChIP\n",
    "sed 1d ${file}/alleleseq_bowtie2/m_min6.txt \\\n",
    "  | awk 'OFS=\"\\t\" { print $2, $3, $4, $1, $21, $22, $23 }' \\\n",
    "  | intersectBed -a stdin -b  ${file}/snpeff/${file}_phased_variants.HQ.pileup_with_WGS -wo \\\n",
    "  > ${file}/alleleseq_bowtie2/BaalChIP/m_min6_with_RAF_for_BaalChIP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rscript BaalChIP_process.R ${sample}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
